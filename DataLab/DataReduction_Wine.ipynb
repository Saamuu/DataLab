{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fd8abe",
   "metadata": {},
   "source": [
    "# The Singular Value Decomposition (SVD)\n",
    "\n",
    "\n",
    "Before we start with the implementation of some applications of the singular value decomposition (SVD), we will briefly introduce the theory behind it.  \n",
    "The SVD is widely used in Data Science and Big Data. The applications are for example  \n",
    "\n",
    "* least-squares regression\n",
    "* model or dimensionality reduction\n",
    "* image compression\n",
    "* principal component analysis\n",
    "* ...\n",
    "---\n",
    "## Some basics\n",
    "Given a matrix $A \\in \\mathbb{R}^{m\\times n}$, we want to compute orthogonal matrices $U \\in \\mathbb{R}^{m\\times m}$ and $V \\in \\mathbb{R}^{n\\times n}$ such that <br>    \n",
    "$$ U^T AV = \\Sigma \\in \\mathbb{R}^{m\\times n}$$  \n",
    "where $\\Sigma$ is a diagonal matrix in a sense that \n",
    "$$\n",
    "     \\Sigma =\\left\\{\\begin{array}{ll} \\begin{pmatrix} \\hat{\\Sigma} \\\\ 0 \\end{pmatrix}, & m \\geq n \\\\\n",
    "         \\left( \\hat{\\Sigma} \\, 0 \\right), & m \\leq n \\end{array}\\right. .\n",
    "$$<br>\n",
    "The matrix $\\hat{\\Sigma} \\in \\mathbb{R}^{p\\times p}$ is a square and diagonal matrix with $p = \\min{(m,n)}$. The diagonal entries are given by $\\sigma_1,...,\\sigma_p$ with <br>  \n",
    "$$ \\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_r > \\sigma_{r+1} = ... = \\sigma_p = 0$$  \n",
    "We call $\\sigma_1,...,\\sigma_r$ the singular values of the matrix $A$.<br> \n",
    "There is a link between the singular values of some matrix $A\\in\\mathbb{R}^{m\\times n}$ and the eigenvalues of the matrix $A^T A \\in \\mathbb{R}^{m\\times m}$ and $A A^T\\in\\mathbb{R}^{n\\times n}$:<br>    \n",
    "$$ \\sigma_j = \\sqrt{\\lambda_j(A^TA)} = \\sqrt{\\lambda_j(AA^T)}, \\, j = 1,...,r. $$ <br> \n",
    "This can be used to compute the singular values of the matrix $A$. We just have to determine the eigenvalues of the matrix $AA^T$ and take the square root of each eigenvalue. Another way to compute the singular values of the matrix $A$ is to use the SVD-algorithm described by Golub, Kahan and Reinsch in 1965. \n",
    "## Why use SVD?\n",
    "We assume that we calculated the SVD of some matrix A with $rank(A) = r$. We can express the matrix as<br>  \n",
    "$$ A = \\sum_{j = 1}^{r} \\sigma_j u_j v_j^T $$<br>  \n",
    "where $u_j \\, j =1,...,m$ are the columns of the matrix $U$ and $v_j \\, j=1,...,n$ are the columns of the matrix $V$.\n",
    "We can define a matrix $A_k$ as<br>  \n",
    "$$ A_k =  \\sum_{j = 1}^{k} \\sigma_j u_j v_j^T $$ <br>  \n",
    "with a $k \\leq r$. We call the matrix $A_k$ the rank k-approximation of the matrix $A$ since $rank(A_k)=k$. <br>   \n",
    "One property of the matrix $A_k$ is that it is the best rank k-approximation of the matrix $A$. This means that for any matrix $B \\in \\mathbb{R}^{m\\times n}$ with $rank(B) \\leq k$, we have<br>  \n",
    "$$ ||A - A_k||_2 \\leq ||A - B||_2$$. <br>  \n",
    "For a more detailed introduction you can have a look at https://en.wikipedia.org/wiki/Singular_value_decomposition (or other sites that introduce the SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b475af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Preparation DataLab:\n",
    "    Code from https://www.kaggle.com/lorenzodenisi/svd-decomposition-and-applications\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce50781",
   "metadata": {},
   "source": [
    "To determine the SVD of a matrix A, we will use the numpy function numpy.linalg.svd(A) (for more information see https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html). The function returns three matrices: <br>\n",
    "* The matrix U that contains the left singular vectors\n",
    "* The vector S that contains the singular values\n",
    "* The matrix V that contains the right singular vectors. <br>\n",
    "We can calculate the matrix A by simply multiplying the three matrices. Since only the singular values are stored (and not the whole matrix S), we have to create a matrix out of the vector. Try to find out how this can be done (https://numpy.org/doc/stable/reference/generated/numpy.diag.html might help).<br>\n",
    "If we want to multiply the matrices, we can use the numpy.matmul function (see https://numpy.org/doc/stable/reference/generated/numpy.matmul.html). Instead of using the numpy.matmul function, we can simply use the \"@\" operator (for example if we want to multiply the matrices A and B we would write A @ B). The \"@\"-operator does the same as the numpy.matmul function. <br>\n",
    "\n",
    "---\n",
    "\n",
    "We now want to get a bit familiar with the SVD-command. For this purpose think of any low-dimensional matrix you want, calculate the SVD of this matrix and multiply the matrices U,S and V. Try to think about a way we can compare those two matrices (for example you could calculate the differences in every matrix component and sum these differences up, see for example https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html, where the frobenius norm is described)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745ea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: Assemble a low-dimensional matrix and calculate the SVD of this matrix\n",
    "\n",
    "\n",
    "#Todo: Multiply the matrices U,S and V and compare the result with the matrix A \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfb38a",
   "metadata": {},
   "source": [
    "The next step is to write a function that gets the full SVD as input and returns the matrices $U_k$ and $V_k$ and the vector $\\Sigma_k$ which are used to calculate the rank k-approximation $A_k$. You can reuse the matrix from the task above to calculate the differences of the matrix A_k and A. If possible you can try to verify the Eckardt-Young-Mirsky Theorem: <br>\n",
    "$$ ||A - A_k||_F^2 = \\sum_{i = k+1}^r \\sigma_{i}^2$$. The term $$||*||_F$$ refers to the so called Frobenius-norm. You might need a matrix of dimension at least 4x4 (You can then truncate two SV and still have two SV left to verify the theorem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec88aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(U, S, V, k):\n",
    "    #Todo: Write the function that truncates tha matrices U,S and V to calculate the rank-k approximation A_k\n",
    "    return U_k,S_k,V_k\n",
    "    \n",
    "# Todo: Calculate the resulting matrix when truncating some SV. Try to verify the Eckardt-Young-Minsky theorem numerically. \n",
    "#       Why are the two results not exactly the same?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ebff88",
   "metadata": {},
   "source": [
    "We can calculate the SVD and the rank-k approximation of a matrix. The next thing we want to have a look at is the dimension reduction of some data. For this purpose we will have a look at a classification task.  <br>\n",
    "Our example will deal with the classification of wine. We have three different classes of wine which are called \"class_0\", \"class_1\" and \"class_2\" in the data set. Additionally we have 178 different bottles of wine that we want to classify. That means that we want to assign each bottle to one of the three classes. To do so we have 13 different features. <br>\n",
    "The first step is to load the data. This is done in the following code snippet. When using the command wine.keys() we have a deeper insight in what we get from loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "260c9b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need this for our example\n",
    "from sklearn.datasets import load_wine\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#load the data\n",
    "wine = load_wine()\n",
    "wine.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a3728",
   "metadata": {},
   "source": [
    "We can see the output of wine.keys(). In the next step we will have a closer look at the data. We find the two fields 'data'and 'target. In these two fields the raw data and the target class of each wine bottle we want to classify. When having a closer look at the 'data' field, we notice that it is basically a 178x13 matrix. This means that for every bottle of wine and every feature of this bottle, one value is stored. <br>\n",
    "This is just some explanation of the data, you should now print out some basic features. Try to find out how the 'data' field is structured and what the feature names are. If you want you can google some of the features and think about how these features can be used to classify the wine bottles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96a847a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: Print/plot the keys from the dataset, understand the structure of the wine object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb5d3d",
   "metadata": {},
   "source": [
    "The above output shows that the data contains different information that is relevant for us:\n",
    "* in wine.data the 13 features of the given wine are stored\n",
    "* in wine.feature_names the name of the feature is stored\n",
    "* in wine.target the classification of the wine is stored\n",
    "* in wine.target_names the name of the class is stored<br>  \n",
    "--- \n",
    "As we understand the data set a little bit better now, we can think about a suitable way to visualize the correlations between the different features. For this purpose we will use scatter plots. This kind of plot is used when we want to investigate the correlations between data. <br>\n",
    "In a scatter plot, different data points with x- and y-corrdinate are displayed. In our case the x-coordinate will be one of the values that a feature from wine bottle 1 takes and the y-coordinate will be the value of another feature from wine-bottle 1. We can store the (x-)values from one feature in a 1x178 array and the (y-)values from another feature in a 1x178 array. We can then use the scatter command from matplotlib (see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) to visualize the corelations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "464297ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Plot some entries of the scatter-matrix (not all since we would get 169 plots which is to much to interpret here)\n",
    "#       Try to interpret your results!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3f843",
   "metadata": {},
   "source": [
    "Until now we didn't reduce the dimension of the problem. We currently use 13 different features to classify the wine. What happens if we use less than 13 features? How can we mathematically determine a way to reduce the dimension of our problem? <br>  \n",
    "This is where the SVD comes in. In the next step you should compute the SVD of the matrix $A \\in \\mathbb{R}^{178\\times 13}$ that contains the data of the wine dataset. Visualize the singular values of the matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f51bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: Calculate SVD\n",
    "\n",
    "\n",
    "#Todo: Plot the SV of the matrix A and decide, how many SV we can truncate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6156129",
   "metadata": {},
   "source": [
    "What features are the most important features when we want to classify the wine? How should we chose our k for the computation of the rank-k Approximation? Again, the Eckardt-Young-Mirsky theorem might help you when thinking about this question. <br>\n",
    "In the next step you should truncate some of the SV and calculate the resulting matrices. Again, you should create scatter plots of the resulting data and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd5cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: Truncate the SV and calculate the truncated matrix A_k (for a suitable k)\n",
    "\n",
    "\n",
    "#Todo: Visualize the data in scatter plots and interpret the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d957e1",
   "metadata": {},
   "source": [
    "The plots above give us an idea of how many SV we can truncate and still be able to classify the wine. The question one may ask is how good some classification algorithms like a Support-Vector-Machine (SVM) work with the reduced data. <br>\n",
    "For this purpose we will use the sklearn.svm package (documentation see https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)(At this point you don't need to know in detail, how a SVM works). <br>\n",
    "To run a classification we first need to split our data set in two groups. One group is for the algorithm to learn how the data should be classified and the other group is to validate the classification after learning. To split the data we will use the function train_test_split from the sklearn.model_selection. This function gets two inputs: The raw data X and the classes y of the data. We an set the size of the groups by an additional parameter (between 0 and 1). If we don't set this parameter, the training set will be 75% of the data and the test set 25%. <br>\n",
    "We will truncate $k=1,...,12$ SV and will have a look at how good the classification works. We will provide you the code for the classification, you just have to figure out how to perform the truncations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c523d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d19e3b8c9828>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0maccTestSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m#Todo: Truncate i SV, calculate U_k*S_k and store the result in the matrix X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Split the data into test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(wine.data), wine.target, random_state=0)\n",
    "\n",
    "#Scale the data (For better results when classifying)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Assemble to arrays that contain accuracies:\n",
    "\n",
    "#Accuracy within the classification of the training data\n",
    "accTrainSet = []\n",
    "#Accuracy when operating on the test data\n",
    "accTestSet = []\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    #Todo: Truncate i SV, calculate U_k*S_k and store the result in the matrix X \n",
    "    \n",
    "    \n",
    "    #Split the data into test and training data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "    \n",
    "    #Scale the data (For better results when classifying)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #Perform the training\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    #Calculate the accuracy\n",
    "    accTrainSet.append(svm.score(X_train,y_train))\n",
    "    accTestSet.append(svm.score(X_test,y_test))\n",
    "\n",
    "#Todo visualize the different achieved accuracies in a suitable plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d6003",
   "metadata": {},
   "source": [
    "Try to interpret the plots that contain the accuracies. How many SV can we truncate and still achieve a \"good\" result when classifying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19005c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
