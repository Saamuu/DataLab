{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fd8abe",
   "metadata": {},
   "source": [
    "# The Singular Value Decomposition (SVD)\n",
    "\n",
    "\n",
    "Before we start with the implementation of some applications of the singular value decomposition (SVD), we will briefly introduce the theory behind it.  \n",
    "The SVD is widely used in Data Science. The applications are for example  \n",
    "\n",
    "* least-squares regression\n",
    "* model or dimensionality reduction\n",
    "* image compression\n",
    "* principal component analysis\n",
    "* ...\n",
    "---\n",
    "## Some basics\n",
    "Given a matrix $A \\in \\mathbb{R}^{m\\times n}$, we want to compute orthogonal matrices $U \\in \\mathbb{R}^{m\\times m}$ and $V \\in \\mathbb{R}^{n\\times n}$ such that <br>    \n",
    "$$ U^T AV = \\Sigma \\in \\mathbb{R}^{m\\times n}$$  \n",
    "where $\\Sigma$ is a diagonal matrix in a sense that \n",
    "$$\n",
    "     \\Sigma =\\left\\{\\begin{array}{ll} \\begin{pmatrix} \\hat{\\Sigma} \\\\ 0 \\end{pmatrix}, & m \\geq n \\\\\n",
    "         \\left( \\hat{\\Sigma} \\, 0 \\right), & m \\leq n \\end{array}\\right. .\n",
    "$$<br>\n",
    "The matrix $\\hat{\\Sigma} \\in \\mathbb{R}^{p\\times p}$ is a square and diagonal matrix with $p = \\min{(m,n)}$. The diagonal entries are given by $\\sigma_1,...,\\sigma_p$ with <br>  \n",
    "$$ \\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_r > \\sigma_{r+1} = ... = \\sigma_p = 0$$  \n",
    "We call $\\sigma_1,...,\\sigma_r$ the singular values (SV) of the matrix $A$. It is possible that $r = p$. <br> \n",
    "There is a link between the singular values of some matrix $A\\in\\mathbb{R}^{m\\times n}$ and the eigenvalues of the matrix $A^T A \\in \\mathbb{R}^{m\\times m}$ and $A A^T\\in\\mathbb{R}^{n\\times n}$:<br>    \n",
    "$$ \\sigma_j = \\sqrt{\\lambda_j(A^TA)} = \\sqrt{\\lambda_j(AA^T)}, \\, j = 1,...,r. $$ <br> \n",
    "This can be used to compute the singular values of the matrix $A$. We just have to determine the eigenvalues of the matrix $AA^T$ and take the square root of each eigenvalue. Another way to compute the singular values of the matrix $A$ is to use the SVD-algorithm described by Golub, Kahan and Reinsch in 1965 (see [here](https://www3.math.tu-berlin.de/Vorlesungen/SS14/MatricesGraphsPDEs/paper_for_students/GolubKahanSVD.pdf) for more information). \n",
    "## Why use SVD?\n",
    "We assume that we calculated the SVD of some matrix A with $rank(A) = r$. We can express the matrix as<br>  \n",
    "$$ A = \\sum_{j = 1}^{r} \\sigma_j u_j v_j^T $$<br>  \n",
    "where $u_j \\, j =1,...,m$ are the columns of the matrix $U$ and $v_j \\, j=1,...,n$ are the columns of the matrix $V$.\n",
    "We can define a matrix $A_k$ as<br>  \n",
    "$$ A_k =  \\sum_{j = 1}^{k} \\sigma_j u_j v_j^T $$ <br>  \n",
    "with a $k \\leq r$. We call the matrix $A_k$ the rank k-approximation of the matrix $A$ since $rank(A_k)=k$. <br>   \n",
    "One property of the matrix $A_k$ is that it is the best rank k-approximation of the matrix $A$. This means that for any matrix $B \\in \\mathbb{R}^{m\\times n}$ with $rank(B) \\leq k$, we have<br>  \n",
    "$$ ||A - A_k||_2 \\leq ||A - B||_2.$$ <br>  \n",
    "For a more detailed introduction you can have a look at [this](https://en.wikipedia.org/wiki/Singular_value_decomposition) (or other sites that introduce the SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b475af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfb38a",
   "metadata": {},
   "source": [
    "## Compute the SVD\n",
    "\n",
    "To determine the SVD of a matrix A, we will use the numpy function numpy.linalg.svd(A) (you can find more information [here]( https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)). The function returns two matrices and a vector: <br>\n",
    "* The matrix U that contains the left singular vectors\n",
    "* The vector S that contains the singular values\n",
    "* The matrix V that contains the right singular vectors. <br>   \n",
    "---\n",
    "We can calculate the matrix A by simply multiplying the three matrices. Since only the singular values are stored (and not the whole matrix S), we have to create a matrix out of the vector. Try to find out how this can be done ([this](https://numpy.org/doc/stable/reference/generated/numpy.diag.html) might help).<br>\n",
    "If we want to multiply the matrices, we can use the numpy.matmul function (see [this](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)). Instead of using the numpy.matmul function, we can simply use the \"@\" operator (for example if we want to multiply the matrices A and B we would write A @ B). The \"@\"-operator does the same as the numpy.matmul function. <br>\n",
    "\n",
    "---\n",
    "\n",
    "Next, we want to get more familiar with the SVD-command. . For this purpose think of any low-dimensional matrix you want, calculate the SVD of this matrix and multiply the matrices U,S and V. Try to think about a way we can compare those two matrices (for example you could calculate the differences in every matrix component and sum these differences up, see for example [here](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html), where the frobenius norm is described)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239deb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: Assemble a low-dimensional matrix and calculate the SVD of this matrix\n",
    "A = [[1,2,3,4],[5,6,7,8],[9,10,12,15],[666,42,2021,10]]\n",
    "U,S,V = np.linalg.svd(A)\n",
    "#Todo: Multiply the matrices U,S and V and compare the result with the matrix A \n",
    "A_ = U @ np.diag(S) @ V\n",
    "\n",
    "fro = np.linalg.norm(A-A_)\n",
    "\n",
    "print(\"The Frobenius-norm of A-A_ is:\",fro)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf334a",
   "metadata": {},
   "source": [
    "## Calculation of the rank-k approximation\n",
    "\n",
    "\n",
    "The next step is to write a function that gets the full SVD as input and returns the matrices $U_k$ and $V_k$ and the vector $\\Sigma_k$ which are used to calculate the rank k-approximation $A_k$. You can reuse the matrix from the task above to calculate the differences of the matrix A_k and A. If possible you can try to verify the Eckardt-Young-Mirsky Theorem: <br>\n",
    "$$ ||A - A_k||_F^2 = \\sum_{i = k+1}^r \\sigma_{i}^2.$$ The term $$||*||_F$$ refers to the so called Frobenius-norm. You might need a matrix of dimension at least 4x4 (You can then truncate two SV and still have two SV left to verify the theorem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(U, S, V, k):\n",
    "    U_trunc = U[:, :k]\n",
    "    S_trunc = S[:k]\n",
    "    V_trunc = V[:k, :]\n",
    "    return U_trunc, S_trunc, V_trunc\n",
    "\n",
    "# Todo: Calculate the resulting matrix when truncating SV. Try to verify the Eckardt-Young-Minsky theorem numerically\n",
    "    \n",
    "A = [[1,2,3,13,4],[13,5,6,7,8],[13,9,10,11,12],[13,13,14,15,16],[13,2,5,2,5]]\n",
    "U,S,V = np.linalg.svd(A)\n",
    "U_,S_,V_ = truncate(U,S,V,2)\n",
    "\n",
    "A_ = U_ @np.diag(S_)@V_\n",
    "\n",
    "normA = np.linalg.norm(A-A_)\n",
    "ref = np.sqrt(S[2]*S[2]+S[3]*S[3]+S[4]*S[4])\n",
    "\n",
    "print(\"The Frobenius norm of A-A_ is:\" ,normA,\"The sum of the squared, trucated SV is:\",ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d06359",
   "metadata": {},
   "source": [
    "## Dimension reduction and wine classification\n",
    "\n",
    "\n",
    "We can calculate the SVD and the rank-k approximation of a matrix. The next topic we want to have a look at is the dimension reduction of some data. For this purpose we will have a look at a classification task. There is a wide range of possible examples for classification tasks. One simple example is the spam filter of your email account that has to decide wheter an incoming email is spam or not. The inspiration for the following tasks can be found [here](https://www.kaggle.com/lorenzodenisi/svd-decomposition-and-applications). <br>\n",
    "In our example we will deal with the classification of wine. In our dataset, we have three different classes of wine which are called \"class_0\", \"class_1\" and \"class_2\". The different classes belong to wine grown in the same region in Italy. [Here](https://archive.ics.uci.edu/ml/datasets/wine), you can find more information about the dataset. Additionally we have 178 different bottles of wine that we want to classify. That means that we want to assign each bottle to one of the three classes. To do so we have 13 different features. <br>\n",
    "The first step is to load the data. This is done in the following code snippet. When using the command wine.keys() we have a deeper insight in what we get from loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#load the data\n",
    "wine = load_wine()\n",
    "wine.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a3728",
   "metadata": {},
   "source": [
    "We can see the output of wine.keys() above. In the next step we will have a closer look at the data. We find the two entries 'data'and 'target. In these two entries the raw data and the target class of each wine bottle we want to classify is stored. When having a closer look at the 'data' entry, we notice that it is basically a 178x13 matrix. This means that for every bottle of wine and every feature of this bottle, one value is stored. <br>\n",
    "This is just some explanation of the data, you should now print some basic features. Try to find out how the dataset is structured and what the feature names are. You can look up some of the features and think about how these features can be used to classify the wine bottles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a847a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the keys from the dataset, for the data chose an arbitrary n\n",
    "n = 10\n",
    "\n",
    "print(wine.data[n,:])\n",
    "print(wine.feature_names)\n",
    "print(wine.target[n])\n",
    "print(wine.target_names)\n",
    "print(np.shape(wine.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb5d3d",
   "metadata": {},
   "source": [
    "The above output should show that the data contains different information that is relevant for our task:\n",
    "* in wine.data the value of the 13 features of the given wine is stored\n",
    "* in wine.feature_names the name of the features are stored\n",
    "* in wine.target the classification of the wine is stored\n",
    "* in wine.target_names the name of the class is stored<br>  \n",
    "--- \n",
    "As we understand the dataset a little bit better now, we can think about a suitable way to visualize the correlations between the different features. For this purpose we will use scatter plots. This kind of plot is used when we want to investigate the correlations between data. <br>\n",
    "In a scatter plot, different data points with x- and y-corrdinate are displayed. In our case the x-coordinate will be one of the values that a feature from wine bottle 1 takes and the y-coordinate will be the value of another feature from wine-bottle 1. We can store the (x-)values from one feature in a 1x178 array and the (y-)values from another feature in a 1x178 array. We can then use the scatter command from matplotlib (see [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html)) to visualize the corelations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464297ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(wine.data)\n",
    "feature_names = wine[\"feature_names\"]\n",
    "data.columns = feature_names\n",
    "data[\"labels\"] = wine.target             \n",
    "\n",
    "# Todo: Plot some entries of the scatter-matrix (not all since we would get 169 plots which is to much to interpret here)\n",
    "#       Interpret your results!\n",
    "featureNum1 = [1,2,12]\n",
    "featureNum2 = [0,2,12]\n",
    "featureNum3 = [0,1,2]\n",
    "fig,axs = plt.subplots(3,3,figsize=(15,15))\n",
    "for i in range(len(featureNum1)):\n",
    "    axs[0,i].scatter(data[feature_names[0]],data[feature_names[featureNum1[i]]],c = data[\"labels\"])\n",
    "    axs[0,i].set_xlabel(feature_names[0])\n",
    "    axs[0,i].set_ylabel(feature_names[featureNum1[i]])\n",
    "    axs[1,i].scatter(data[feature_names[1]],data[feature_names[featureNum2[i]]],c = data[\"labels\"])\n",
    "    axs[1,i].set_xlabel(feature_names[1])\n",
    "    axs[1,i].set_ylabel(feature_names[featureNum2[i]])\n",
    "    axs[2,i].scatter(data[feature_names[12]],data[feature_names[featureNum3[i]]],c = data[\"labels\"]) \n",
    "    axs[2,i].set_xlabel(feature_names[2])\n",
    "    axs[2,i].set_ylabel(feature_names[featureNum3[i]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3f843",
   "metadata": {},
   "source": [
    "Until now we didn't reduce the dimension of the problem. We currently use 13 different features to classify the wine. What happens if we use less than 13 features? How can we mathematically determine a way to reduce the dimension of our problem? <br>  \n",
    "This is where the SVD comes into play. In the next step you should compute the SVD of the matrix $A \\in \\mathbb{R}^{178\\times 13}$ that contains the data of the wine dataset. Visualize the singular values of the matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f51bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: Calculate SVD\n",
    "\n",
    "A = data[feature_names].values\n",
    "\n",
    "U, S, Vt = np.linalg.svd(A)\n",
    "\n",
    "#Todo: Plot the SV of the matrix A and decide, how many SV we can truncate\n",
    "fig,axs = plt.subplots()\n",
    "axs.barh(feature_names[::-1], S[::-1])\n",
    "axs.set_title(f\"Singular values\")\n",
    "axs.set_xscale('log')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6156129",
   "metadata": {},
   "source": [
    "What features are the most important features when we want to classify the wine? How should we chose our k for the computation of the rank-k Approximation? Again, the Eckardt-Young-Mirsky theorem might help you when thinking about this question. <br>\n",
    "In the next step you should truncate some of the SV and calculate the resulting matrices. Again, you should create scatter plots of the resulting data and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: Truncate the SV and calculate the truncated matrix A_k (for a suitable k)\n",
    "k = 3\n",
    "\n",
    "A = wine.data[np.isin(wine.target,[0,1,2])]\n",
    "\n",
    "target=wine.target[np.isin(wine.target,[0,1,2])]\n",
    "\n",
    "U, S, Vt = np.linalg.svd(A)\n",
    "\n",
    "\n",
    "U_trunc, S_trunc, Vt_trunc = truncate(U, S, Vt, k)\n",
    "reduced_data = U_trunc @ np.diag(S_trunc)\n",
    "trunc_A = pd.DataFrame(reduced_data)\n",
    "\n",
    "\n",
    "\n",
    "#Todo: Visualize the data in scatter plots and interpret the results\n",
    "fig,ax=plt.subplots(figsize=(5, 5))\n",
    "for wineClass,colour in [(0,'r'),(1,'b'),(2,'g')]:\n",
    "    data=reduced_data[target==wineClass]\n",
    "    ax.scatter(data[:,0], data[:,1],c = colour,alpha=0.5,edgecolor='k',label=\"Class {}\".format(wineClass))\n",
    "    ax.set_xlabel(\"First feature\")\n",
    "    ax.set_ylabel(\"Second feature\")\n",
    "    ax.legend()\n",
    "\n",
    "# not as good as expected when using 3 features, let's try and have a look at them seperately\n",
    "fig,ax=plt.subplots(figsize=(5, 5))\n",
    "for wineClass,colour in [(0,'r'),(1,'b')]:\n",
    "    data=reduced_data[target==wineClass]\n",
    "    ax.scatter(data[:,0], data[:,1],c = colour,alpha=0.5,edgecolor='k',label=\"Class {}\".format(wineClass))\n",
    "    ax.set_xlabel(\"First feature\")\n",
    "    ax.set_ylabel(\"Second feature\")\n",
    "    ax.legend()\n",
    "    \n",
    "fig,ax=plt.subplots(figsize=(5, 5))\n",
    "for wineClass,colour in [(0,'r'),(2,'g')]:\n",
    "    data=reduced_data[target==wineClass]\n",
    "    ax.scatter(data[:,0], data[:,1],c = colour,alpha=0.5,edgecolor='k',label=\"Class {}\".format(wineClass))\n",
    "    ax.set_xlabel(\"First feature\")\n",
    "    ax.set_ylabel(\"Second feature\")\n",
    "    ax.legend()\n",
    "    \n",
    "fig,ax=plt.subplots(figsize=(5, 5))\n",
    "for wineClass,colour in [(1,'b'),(2,'g')]:\n",
    "    data=reduced_data[target==wineClass]\n",
    "    ax.scatter(data[:,0], data[:,1],c = colour,alpha=0.5,edgecolor='k',label=\"Class {}\".format(wineClass))\n",
    "    ax.set_xlabel(\"First feature\")\n",
    "    ax.set_ylabel(\"Second feature\")\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ac851",
   "metadata": {},
   "source": [
    "Alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75671338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting._misc import scatter_matrix\n",
    "from matplotlib import cm\n",
    "    \n",
    "\n",
    "X = pd.DataFrame(wine.data)\n",
    "print(wine[\"feature_names\"])\n",
    "y = wine.target\n",
    "cmap = cm.get_cmap('gnuplot')\n",
    "scatter = scatter_matrix(X, c = y, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap = cmap)\n",
    "plt.suptitle('Scatter-matrix for each input variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d957e1",
   "metadata": {},
   "source": [
    "The plots above give us an idea of how many SV we can truncate and still be able to classify the wine. The question one may ask is how good some classification algorithms like a Support-Vector-Machine (SVM) work with the reduced data. <br>\n",
    "For this purpose we will use the sklearn.svm package (documentation see [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm))(At this point you don't need to know in detail, how a SVM works). <br>\n",
    "To run a classification we first need to split our data set in two groups. One group is for the algorithm to learn how the data should be classified and the other group is to validate the classification after learning. To split the data we will use the function train_test_split from the sklearn.model_selection. This function gets two inputs: The raw data X and the classes y of the data. We an set the size of the groups by an additional parameter (between 0 and 1). If we don't set this parameter, the training set will be 75% of the data and the test set 25%. <br>\n",
    "We will truncate $k=1,...,12$ SV and will have a look at how good the classification works. We will provide you the code for the classification, you just have to figure out how to perform the truncations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c523d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "X = pd.DataFrame(wine.data)\n",
    "y = wine.target\n",
    "#Split the data into test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "#Scale the data (For better results when classifying)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Assemble to arrays that contain accuracies:\n",
    "\n",
    "#Accuracy within the classification of the training data\n",
    "accTrainSet = []\n",
    "#Accuracy when operating on the test data\n",
    "accTestSet = []\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    #Todo: Truncate i SV, calculate U_k*S_k and store the result in the matrix X \n",
    "    k = 13-i\n",
    "    U_trunc, S_trunc, Vt_trunc = truncate(U, S, Vt, k)\n",
    "    X = pd.DataFrame(U_trunc @ np.diag(S_trunc))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    accTrainSet.append(svm.score(X_train,y_train))\n",
    "    accTestSet.append(svm.score(X_test,y_test))\n",
    "\n",
    "fig,axs = plt.subplots(1,2)\n",
    "axs[0].plot(accTrainSet)\n",
    "axs[0].set_xlabel(\"Number of truncated SV\")\n",
    "axs[0].set_ylabel(\"Accuracy on Training Set\")\n",
    "\n",
    "axs[1].plot(accTestSet)\n",
    "axs[1].set_xlabel(\"Number of truncated SV\")\n",
    "axs[1].set_ylabel(\"Accuracy on Test Set\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc5000",
   "metadata": {},
   "source": [
    "Interpret the plots that contain the accuracies. How many SV can we truncate and still achieve a \"good\" result when classifying?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
